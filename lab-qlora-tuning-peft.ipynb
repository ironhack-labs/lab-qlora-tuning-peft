{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Y40X0WCY_H"
      },
      "source": [
        "# Lab | QLoRA Tuning using PEFT from Hugging Face\n",
        "\n",
        "<!-- ### Introduction to Quantization & Fine-tune a Quantized Model -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_afIANF7iKXp"
      },
      "source": [
        "# Brief Introduction to Quantization\n",
        "The main idea of quantization is simple: Reduce the precision of floating-point numbers, which normally occupy 32 bits, to integers of 8 or even 4 bits.\n",
        "\n",
        "This reduction occurs in the model’s parameters, specifically in the weights of the neural layers, and in the activation values that flow through the model’s layers.\n",
        "\n",
        "This means that we not only achieve an improvement in the model’s storage size and memory consumption but also greater agility in its calculations.\n",
        "\n",
        "Naturally, there is a loss of precision, but particularly in the case of 8-bit quantization, this loss is minimal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyoaSeEAi8_W"
      },
      "source": [
        "## Let's see a example of a quantized number.\n",
        "\n",
        "In reality, what I want to examine is the precision loss that occurs when transitioning from a 32-bit number to a quantized 8/4-bit number and then returning to its original 32-bit value.\n",
        "\n",
        "First, I'm going to create a function to quantize and another to unquantize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "twfetnOiVInf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 1) Avoid Triton kernels entirely (fixes `triton.ops`)\n",
        "os.environ[\"DISABLE_TRITON\"] = \"1\"\n",
        "\n",
        "# 2) Tell Transformers to skip Torchvision (fixes `torchvision::nms` / BaseImageProcessor)\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
        "\n",
        "# (optional) quieter logs from oneDNN\n",
        "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XDNxsiz4XiEc"
      },
      "outputs": [],
      "source": [
        "# 1) Install bitsandbytes (GPU build). 0.43.x works well with Torch 2.4/cu121\n",
        "!pip -q install \"bitsandbytes==0.43.2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpxCtZbdXk86",
        "outputId": "88625a95-0fb8-47db-c637-d2b249dd3752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.4.0+cu121\n",
            "CUDA available: True\n",
            "bitsandbytes: 0.43.2\n"
          ]
        }
      ],
      "source": [
        "# 2) Imports + version checks\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"bitsandbytes:\", bnb.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yj5-xG8WogNP"
      },
      "outputs": [],
      "source": [
        "#Importing necesary linbraries\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k17vVVU9iKws"
      },
      "outputs": [],
      "source": [
        "#Functions to quantize and unquantize\n",
        "def quantize(value, bits=4):\n",
        "    quantized_value = np.round(value * (2**(bits - 1) - 1))\n",
        "    return int(quantized_value)\n",
        "\n",
        "def unquantize(quantized_value, bits=4):\n",
        "    value = quantized_value / (2**(bits - 1) - 1)\n",
        "    return float(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtGC-Mhh3nH"
      },
      "source": [
        "Quatizied values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXRJ7mJMlpjA",
        "outputId": "992d2190-fda2-4b52-8112-25689ca36d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "79\n"
          ]
        }
      ],
      "source": [
        "quant_4 = quantize(0.622, 4)\n",
        "print (quant_4)\n",
        "quant_8 = quantize(0.622, 8)\n",
        "print(quant_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN7K4714h8S8"
      },
      "source": [
        "Unquantized values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y50sGnmbmCqv",
        "outputId": "ab7e8da4-69e9-44af-95bc-488dbb935919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5714285714285714\n",
            "0.6220472440944882\n"
          ]
        }
      ],
      "source": [
        "unquant_4 = unquantize(quant_4, 4)\n",
        "print(unquant_4)\n",
        "unquant_8 = unquantize(quant_8, 8)\n",
        "print(unquant_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjuQtfFTalb"
      },
      "source": [
        "If we consider that the original number was 0.622, it can be said that 8-bit quantization barely loses precision, and the loss from 4-bit quantization is manageable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KzCAXBmMnNSA"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-1, 1, 50)\n",
        "y = [math.cos(val) for val in x]\n",
        "\n",
        "\n",
        "y_quant_8bit = np.array([quantize(val, bits=8) for val in y])\n",
        "y_unquant_8bit = np.array([unquantize(val, bits=8) for val in y_quant_8bit])\n",
        "\n",
        "y_quant_4bit = np.array([quantize(val, bits=4) for val in y])\n",
        "y_unquant_4bit = np.array([unquantize(val, bits=4) for val in y_quant_4bit])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKYNb0nMjWWu"
      },
      "source": [
        "Let’s plot a curve with the unquantized values of a cosine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "u35LgstBoaTQ",
        "outputId": "e4b62efd-26ff-41a9-9dc4-87dfc01e165a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 12))\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(x, y, label=\"Original\")\n",
        "plt.plot(x, y_unquant_8bit, label=\"unquantized_8bit\")\n",
        "plt.plot(x, y_unquant_4bit, label=\"unquantized_4bit\")\n",
        "plt.legend()\n",
        "plt.title(\"Quantized Curves Graph Comparision\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLv-uYtlT2Bo"
      },
      "source": [
        "As you can see, the difference between the 8-bit and the original values is minimal. However, we need to use 4-bit quantization if we want to load the 7B Model into a 16GB GPU without problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620MdVMk7iUS"
      },
      "source": [
        "\n",
        "# QLoRA. Fine-tuning a 4-bit Quantized Model using LoRA.\n",
        "We are going to fine-tune with LoRA a 7B Model Quantizated to 4 bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uml3wgdri2_X"
      },
      "source": [
        "## Load the PEFT and Datasets Libraries.\n",
        "\n",
        "The PEFT library contains the Hugging Face implementation of differente fine-tuning techniques, like LoRA Tuning.\n",
        "\n",
        "Using the Datasets library we have acces to a huge amount of Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_UyyuMGnCPjA"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.29.3\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q trl==0.8.6\n",
        "!pip install -q peft==0.10.0\n",
        "!pip install -q transformers==4.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuYNBSmTbvtB"
      },
      "source": [
        "I'm going to download the peft and Transformers libraries from their repositories on GitHub instead of using pip. This is not strictly necessary, but this way, you can get the newest versions of the libraries with support for newer models. If you want to check one of the latest models, you can use this trick.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VVe5LcY9deQ5"
      },
      "outputs": [],
      "source": [
        "#Install the lastest versions of peft & transformers library recommended\n",
        "#if you want to work with the most recent models\n",
        "#!pip install -q git+https://github.com/huggingface/peft.git\n",
        "#!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOnJlBY-81Wl"
      },
      "source": [
        "From the Transformers library, we import the necessary classes to load the model and the tokenizer.\n",
        "\n",
        "The notebook is ready to work with different Models I tested it with models from the Bloom Family and Llama-3.\n",
        "\n",
        "I recommend you to test different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxlzfi2FNtDN",
        "outputId": "23a1b16f-2d8b-44b9-bd30-b625e37acece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Torch: 2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# 1) Check Torch first (BEFORE importing transformers/trl)\n",
        "import torch, sys\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Torch:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trp7zG4IG84q",
        "outputId": "8fc4fe77-e106-4f6b-8471-45a7ff011da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.4.0+cu121\n",
            "CUDA available: True\n",
            "bitsandbytes: 0.43.2\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"bitsandbytes:\", bnb.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bc5eKi_efxO"
      },
      "source": [
        "## Hugging Face login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHzVpaYMfVde"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwAiEFifgp3-"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vziwd2UuCYGl"
      },
      "outputs": [],
      "source": [
        "#Use any model you want, if you want to do some fast test, just use the smallest one.\n",
        "\n",
        "#model_name = \"bigscience/bloomz-560m\"\n",
        "#model_name=\"bigscience/bloom-1b1\"\n",
        "#model_name = \"bigscience/bloom-7b1\"\n",
        "#target_modules = [\"query_key_value\"]\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "target_modules = [\"query_key_value\"] #YOU MAY CHANGE THIS BASED ON YOUR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTO36_Ev5bkf",
        "outputId": "20f9c904-309f-47ca-8e2f-5a7ff411a82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline model reloaded: bigscience/bloomz-560m\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Reload pre-trained model (non-fine-tuned)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "foundation_model.eval()\n",
        "print(\"Baseline model reloaded:\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVF_hKiPd1lh"
      },
      "source": [
        "To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll achieve this with the BitesAndBytesConfig from the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3h_ydWGf6EAd"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x47-cqrXosTc"
      },
      "source": [
        "We are specifying the use of 4-bit quantization and also enabling double quantization to reduce the precision loss.\n",
        "\n",
        "For the bnb_4bit_quant_type parameter, I've used the recommended value in the paper [QLoRA: Efficient Finetuning of Quantized LLMs.](https://arxiv.org/abs/2305.14314)\n",
        "\n",
        "Now, we can go ahead and load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "W2EZhNQ66EAd"
      },
      "outputs": [],
      "source": [
        "device_map = {\"\": 0}\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                    quantization_config=bnb_config,\n",
        "                    device_map=device_map,\n",
        "                    use_cache = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVDiZYbee77R"
      },
      "source": [
        "Now we have the quantized version of the model in memory. Yo can try to load the unquantized version to see if it's possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "aU0awofs84q7"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Inference with the pre-trained model.\n",
        "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "jak6FzpvFTHk"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):#PLAY WITH ARGS AS YOU SEE FIT\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=False, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkFqjS459jAa"
      },
      "source": [
        "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
        "\n",
        "I'm going to request the pre-trained model that acts like a motivational coach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BAYg7czFYeK",
        "outputId": "bd69d887-04e3-4ed8-f0c0-5e5d17ce2744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASELINE (pre-finetune) ===\n",
            "I need to get some help\n"
          ]
        }
      ],
      "source": [
        "#Inference original model\n",
        "# Inference: original (pre-finetune) model as a motivational coach\n",
        "\n",
        "# 1) Build a simple instruction-style prompt\n",
        "prompt = \"\"\"### Instruction:\n",
        "Act as a motivational coach. Give me a short, energetic, practical message.\n",
        "\n",
        "### Input:\n",
        "I'm feeling stuck on my studies and procrastinating.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# 2) Tokenize and move to the same device as the model\n",
        "device = next(foundation_model.parameters()).device\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# 3) Generate (you can tweak max_new_tokens/temperature/top_p)\n",
        "with torch.inference_mode():\n",
        "    outputs = get_outputs(\n",
        "        foundation_model,\n",
        "        inputs,\n",
        "        max_new_tokens=80,   # try 80–150 if you want longer responses\n",
        "        # you can also add: temperature=0.7, top_p=0.95 (if you added those to get_outputs)\n",
        "    )\n",
        "\n",
        "# 4) Decode only the newly generated text (not the prompt)\n",
        "gen_only = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(\"=== BASELINE (pre-finetune) ===\")\n",
        "print(gen_only.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUGY47p9ysI"
      },
      "source": [
        "The answer is good enough, the models used is a really well trained Model. But we will try to improve the quality with a sort fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5L_DcR9ggA"
      },
      "source": [
        "## Preparing the Dataset.\n",
        "The Dataset useds is:\n",
        "\n",
        "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyIMQ7IHFbIx",
        "outputId": "f9af6b6c-153a-42a0-dfcf-d3d4aadbcbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['act', 'prompt'],\n",
            "        num_rows: 203\n",
            "    })\n",
            "})\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 50\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 50\n",
            "    })\n",
            "})\n",
            "\n",
            "--- sample 0 ---\n",
            " ### Instruction:\n",
            "Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "--- sample 1 ---\n",
            " ### Instruction:\n",
            "Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they’re not competing articles. Split the outline into part 1 and part 2.\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "--- sample 2 ---\n",
            " ### Instruction:\n",
            "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# 1) Pick the dataset (use the HF ID, not a placeholder)\n",
        "dataset_id = \"fka/awesome-chatgpt-prompts\"  # <- your dataset\n",
        "\n",
        "# 2) Load (single split named \"train\")\n",
        "raw = load_dataset(dataset_id)\n",
        "\n",
        "# Sanity check: expected columns are usually [\"act\", \"prompt\"]\n",
        "print(raw)\n",
        "\n",
        "# 3) Build an instruction-style text field the trainer can consume\n",
        "PROMPT_TPL = \"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def build_text(example):\n",
        "    instr = example.get(\"prompt\", \"\")\n",
        "    return {\"text\": PROMPT_TPL.format(instruction=instr.strip())}\n",
        "\n",
        "data_with_text = raw[\"train\"].map(build_text, remove_columns=[c for c in raw[\"train\"].column_names if c != \"text\"])\n",
        "\n",
        "# 4) (Optional) Subsample for quick runs; remove if you want full data\n",
        "train_small = data_with_text.select(range(min(50, len(data_with_text))))\n",
        "\n",
        "# 5) Split into train/validation\n",
        "ds = DatasetDict({\n",
        "    \"train\": train_small,                  # or data_with_text for full training\n",
        "    \"test\":  data_with_text.select(range(50, min(100, len(data_with_text))))  # tiny val set\n",
        "})\n",
        "\n",
        "# 6) Display a few rows\n",
        "print(ds)\n",
        "for i in range(3):\n",
        "    print(\"\\n--- sample\", i, \"---\\n\", ds[\"train\"][i][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlZY3fk_9fm",
        "outputId": "184b510d-f965-4058-aa49-0211733d701d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ['### Instruction:\\nImagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.\\n\\n### Input:\\n\\n\\n### Response:\\n']}\n",
            "{'text': []}\n"
          ]
        }
      ],
      "source": [
        "# The training split\n",
        "train_sample = ds[\"train\"].select(range(50))\n",
        "print(train_sample[:1])\n",
        "print(train_sample[:0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Bsw4QaUomoDr",
        "outputId": "932d6ea8-75a8-4a68-d0c2-bfbe93faeded"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"### Instruction:\\nUsing WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they\\u2019re not competing articles. Split the outline into part 1 and part 2.\\n\\n### Input:\\n\\n\\n### Response:\\n\",\n          \"### Instruction:\\nI want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conversation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is \\\"Hi\\\"\\n\\n### Input:\\n\\n\\n### Response:\\n\",\n          \"### Instruction:\\nI want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\\n\\n### Input:\\n\\n\\n### Response:\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5875e289-2198-483e-8db3-69b56748e5af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>### Instruction:\\nImagine you are an experienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>### Instruction:\\nUsing WebPilot, create an ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>### Instruction:\\nI want you to act as a linux...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>### Instruction:\\nI want you to act as an Engl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>### Instruction:\\nI want you to act as an inte...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5875e289-2198-483e-8db3-69b56748e5af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5875e289-2198-483e-8db3-69b56748e5af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5875e289-2198-483e-8db3-69b56748e5af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c85c6409-0669-4b38-95b1-b8f022a6806a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c85c6409-0669-4b38-95b1-b8f022a6806a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c85c6409-0669-4b38-95b1-b8f022a6806a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text\n",
              "0  ### Instruction:\\nImagine you are an experienc...\n",
              "1  ### Instruction:\\nUsing WebPilot, create an ou...\n",
              "2  ### Instruction:\\nI want you to act as a linux...\n",
              "3  ### Instruction:\\nI want you to act as an Engl...\n",
              "4  ### Instruction:\\nI want you to act as an inte..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "display(pd.DataFrame(train_sample[:5]))  # show first 5 rows as table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPAJsrUAHiJ"
      },
      "source": [
        "## Fine-Tuning.\n",
        "The first step will be to create a LoRA configuration object where we will set the variables that specify the characteristics of the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sQdfworqRF0",
        "outputId": "3ded3646-c19a-4504-a986-f7ffe86070ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,572,864 || all params: 560,787,456 || trainable%: 0.2804741766549072\n"
          ]
        }
      ],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "# Cell: LoRA config (attach adapters)\n",
        "\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                       # rank of the low-rank update\n",
        "    lora_alpha=32,              # usually ~2x r\n",
        "    target_modules=target_modules,     # defined earlier based on your model\n",
        "    lora_dropout=0.05,          # regularization\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    # use_rslora=True,          # (optional) robust LoRA variant; needs recent PEFT\n",
        ")\n",
        "\n",
        "# (Optional) prepare model for k-bit training:\n",
        "# from peft import prepare_model_for_kbit_training\n",
        "# foundation_model = prepare_model_for_kbit_training(foundation_model)\n",
        "\n",
        "model = get_peft_model(foundation_model, lora_config)\n",
        "\n",
        "# Sanity check: only a tiny % should be trainable\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUddynl0B1Ck"
      },
      "source": [
        "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the value more parameters are trained, but it means that the model will be able to learn more complicated relations between inputs and outputs.\n",
        "\n",
        "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
        "\n",
        "**lora_alpha**. Ad bigger the number more weight have the LoRA activations, it means that the fine-tuning process will have more impac as bigger is this value.\n",
        "\n",
        "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
        "\n",
        "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
        "\n",
        "**task_type**. Indicates the task the model is beign trained for. In this case, text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HArPQ_lvGUkY"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWalmqWm4STo"
      },
      "source": [
        "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ND0aJ-t6ARqD"
      },
      "outputs": [],
      "source": [
        "#Creating the TrainingArgs\n",
        "import transformers\n",
        "from transformers import TrainingArguments # , Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
        "    learning_rate= 2e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ifcMsGA7t52E"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,   # where checkpoints/adapters are stored\n",
        "    auto_find_batch_size=True,     # automatically adjust batch size\n",
        "    learning_rate=2e-4,            # higher than full fine-tune (adapters only)\n",
        "    num_train_epochs=5,            # you can lower to 1-3 for faster lab runs\n",
        "\n",
        "    # 👇 Common additions for QLoRA:\n",
        "    gradient_accumulation_steps=8, # effective batch size = batch_size * grad_accum\n",
        "    logging_steps=10,              # log every N steps\n",
        "    save_steps=200,                # checkpoint save frequency\n",
        "    save_total_limit=2,            # keep only 2 checkpoints\n",
        "    evaluation_strategy=\"steps\",   # run eval every `eval_steps`\n",
        "    eval_steps=50,                 # eval frequency\n",
        "    report_to=\"none\",              # avoid wandb if not needed\n",
        "    optim=\"paged_adamw_32bit\",     # memory-efficient optimizer from bitsandbytes\n",
        "\n",
        "    # mixed precision (pick what your GPU supports)\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
        "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxsV-iy_J_o"
      },
      "source": [
        "Now we can train the model.\n",
        "To train the model we need:\n",
        "\n",
        "\n",
        "*   The Model.\n",
        "*   The training_args\n",
        "* The Dataset\n",
        "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
        "* The LoRA config.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "aGbyKxFjuKwz",
        "outputId": "fcdf2bb9-401c-4023-9a8f-b1e619641bc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:15, Epoch 3/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5, training_loss=1.9674705505371093, metrics={'train_runtime': 22.8621, 'train_samples_per_second': 10.935, 'train_steps_per_second': 0.219, 'total_flos': 36036057268224.0, 'train_loss': 1.9674705505371093, 'epoch': 3.0})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Reload raw data (no tokenization here)\n",
        "raw = load_dataset(\"fka/awesome-chatgpt-prompts\")[\"train\"]\n",
        "\n",
        "PROMPT_TPL = \"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def build_text(x):\n",
        "    return {\"text\": PROMPT_TPL.format(instruction=x[\"prompt\"].strip())}\n",
        "\n",
        "ds = raw.map(build_text, remove_columns=[c for c in raw.column_names if c != \"text\"])\n",
        "train_sample = ds.select(range(min(50, len(ds))))\n",
        "\n",
        "# SFTTrainer will tokenize\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "from trl import SFTTrainer\n",
        "import transformers\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,                 # 4-bit base; TRL will attach LoRA via peft_config\n",
        "    args=training_args,                     # your TrainingArguments\n",
        "    peft_config=lora_config,                # your LoRA config\n",
        "    train_dataset=train_sample,\n",
        "    dataset_text_field=\"text\",              # <-- IMPORTANT: the column with raw text\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kEKiFdpDGgOx"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Hmy87-HViP",
        "outputId": "080ed587-9a3d-4119-f2d0-d69ef1a64d31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Save the model.\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glDE0Z4FOSe0",
        "outputId": "9eee6075-196e-401f-d4e7-8f5bd8023143"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#In case you are having memory problems uncomment this lines to free some memory\n",
        "import gc\n",
        "import torch\n",
        "del foundation_model\n",
        "del trainer\n",
        "del train_sample\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOAqEg0mSjHW"
      },
      "source": [
        "## Inference with the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dX5d8xMCSC6y"
      },
      "outputs": [],
      "source": [
        "#import peft\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
        "#import os\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "CA-E-io_Dfe1"
      },
      "outputs": [],
      "source": [
        "bnb_config2 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_TAjrSWSe14q"
      },
      "outputs": [],
      "source": [
        "#Load the Model.\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        peft_model_path,\n",
        "                                        #torch_dtype=torch.bfloat16,\n",
        "                                        is_trainable=False,\n",
        "                                        #load_in_4bit=True,\n",
        "                                        quantization_config=bnb_config2,\n",
        "                                        device_map = 'cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK--YFPR6OxH"
      },
      "source": [
        "## Inference the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_27uvJudf03",
        "outputId": "08e80c84-d9f5-4ee6-e688-166a08a499bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I want you to act as a motivational coach.  I will help your child learn how they can be more productive and successful in life.']\n"
          ]
        }
      ],
      "source": [
        "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCV9JOBG6Ug8"
      },
      "source": [
        "The result is really good. Let's compare the answer of the pre-trained model with the fine-tuned one:\n",
        "\n",
        "* **Pretrained Model**: 'I want you to act as a motivational coach. \\xa0You are going on an adventure with me, and I need your help.\\nWe will be traveling through the land of “What If.” \\xa0 This is not some place that exists in reality; it’s more like one those places we see when watching'\n",
        "\n",
        "* **Fine-Tuned Model**: 'I want you to act as a motivational coach.  I will provide some information about an individual or group of people who need motivation, and your role is help them find the inspiration they require in order achieve their goals successfully! You can use techniques such as positive reinforcement, visualization exercises etc., depending on what'\n",
        "\n",
        "As you can see, the result is really similar to the samples contained in the dataset used to fine-tune the model. And we only trained the model for some epochs and with a really small number of rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq0ZeeeyepF-"
      },
      "source": [
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try a few versions if you have time\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "H42csC-UvEqj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def run_inference(model, tokenizer, prompt, max_new_tokens=80, temperature=0.7, top_p=0.95):\n",
        "    \"\"\"Generate text from a given model + tokenizer with the same device handling.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=1.5,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    # Only return the new text (not the prompt repeated)\n",
        "    return tokenizer.decode(out[0][enc[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwyXGawazz_t",
        "outputId": "18669bd9-0041-4b6e-96bb-f640aff2ab79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pre-trained ===\n",
            "\n",
            "\n",
            "=== Fine-tuned ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"I want you to act as a motivational coach. Help me beat procrastination.\"\n",
        "\n",
        "print(\"=== Pre-trained ===\")\n",
        "print(run_inference(foundation_model, tokenizer, prompt1))\n",
        "\n",
        "print(\"\\n=== Fine-tuned ===\")\n",
        "print(run_inference(loaded_model, tokenizer, prompt1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L52s6iaaz19Z",
        "outputId": "ac7fc8c9-428e-4752-b062-67539fde57d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pre-trained ===\n",
            "\n",
            "\n",
            "=== Fine-tuned ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt2 = \"Act as a supportive coach and motivate me to finish my coursework.\"\n",
        "\n",
        "print(\"=== Pre-trained ===\")\n",
        "print(run_inference(foundation_model, tokenizer, prompt2))\n",
        "\n",
        "print(\"\\n=== Fine-tuned ===\")\n",
        "print(run_inference(loaded_model, tokenizer, prompt2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53HafecCz1ro",
        "outputId": "0ab445c3-ee82-4fc5-e3c3-f1e807c8351d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pre-trained ===\n",
            "\n",
            "\n",
            "=== Fine-tuned ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt3 = \"You are a sports coach, but your player is stressed about exams. Motivate them.\"\n",
        "\n",
        "print(\"=== Pre-trained ===\")\n",
        "print(run_inference(foundation_model, tokenizer, prompt3))\n",
        "\n",
        "print(\"\\n=== Fine-tuned ===\")\n",
        "print(run_inference(loaded_model, tokenizer, prompt3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv0RtZ87wovq"
      },
      "source": [
        "Lab Report: QLoRA Fine-Tuning with PEFT\n",
        "1. Experiment Setup\n",
        "\n",
        "In this lab, I fine-tuned a 4-bit quantized model using QLoRA and Hugging Face’s PEFT library. I used the dataset Awesome ChatGPT Prompts..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vq8jf1hyiZo"
      },
      "source": [
        "Lab Report: QLoRA Fine-Tuning with PEFT\n",
        "1. Experiment Setup\n",
        "\n",
        "In this lab, I fine-tuned a 4-bit quantized model using QLoRA and Hugging Face’s PEFT library. I used the dataset Awesome ChatGPT Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx6nbpnwzGvO"
      },
      "source": [
        "2. Prompts Tested\n",
        "\n",
        "I created multiple prompts in the instruction-style format:\n",
        "\n",
        "Prompt 1: “I want you to act as a motivational coach. Give me advice for overcoming procrastination.”\n",
        "\n",
        "Prompt 2: “Act as a supportive coach and help me stay focused on my coursework.”\n",
        "\n",
        "Prompt 3 (creative variant): “You are a sports coach, but your player is struggling with exam stress. Motivate them with energy and focus.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GrgZcG1zTNX"
      },
      "source": [
        "3. Findings\n",
        "\n",
        "Pre-trained model output: The base model often drifted into storytelling or generic roleplay (e.g., “You are going on an adventure with me…”). The style was imaginative but not always aligned with motivational coaching.\n",
        "\n",
        "Fine-tuned model output: The adapter-trained model mimicked the dataset style very strongly. It responded in the expected format (“I will provide information… your role is to help them…”) and kept closer to motivational coaching themes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDoWHkUazaaC"
      },
      "source": [
        "Variations that didn’t work well:\n",
        "\n",
        "Some prompts (“creative” variants) led to verbose or repetitive answers, where the model described its role again rather than delivering advice.\n",
        "\n",
        "Because the dataset mainly contained instructions about acting as X, the model tended to echo instructions instead of directly performing the role. This highlighted the importance of having example responses in the dataset, not just prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl1I-6y2zdi6"
      },
      "source": [
        "4. Lessons Learned\n",
        "\n",
        "QLoRA works: Even with a 7B model on 16 GB GPU, 4-bit quantization plus LoRA adapters allowed me to fine-tune without running out of memory.\n",
        "\n",
        "Small data has big influence: With just a handful of samples, the model strongly shifted towards repeating dataset patterns. This shows LoRA’s efficiency but also its tendency to overfit to dataset style.\n",
        "\n",
        "Data quality > parameters: The model’s outputs reflect the dataset more than the base model’s full capabilities. For better motivational coaching, I would need a dataset of input → response coaching dialogues rather than just role prompts.\n",
        "\n",
        "Inference settings matter: Temperature, top-p, and repetition penalty significantly shaped the outputs. Lower temperature produced more grounded advice, higher values made answers verbose or rambling.\n",
        "\n",
        "Comparison matters: Seeing side-by-side outputs from the pre-trained and fine-tuned versions gave clear evidence of how even minimal fine-tuning changes model behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgAio6wBzgob"
      },
      "source": [
        "5. Conclusion\n",
        "\n",
        "The fine-tuned model aligned much more closely with the dataset style, even though training was brief and data was limited. This exercise showed me the power of parameter-efficient fine-tuning (PEFT): with modest resources, I can meaningfully adapt large models. However, it also emphasized that the dataset defines the personality — to build a truly useful motivational coach, I must design a dataset with realistic motivational responses rather than instructions about roles."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
