{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Y40X0WCY_H"
      },
      "source": [
        "# Lab | QLoRA Tuning using PEFT from Hugging Face\n",
        "\n",
        "<!-- ### Introduction to Quantization & Fine-tune a Quantized Model -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_afIANF7iKXp"
      },
      "source": [
        "# Brief Introduction to Quantization\n",
        "The main idea of quantization is simple: Reduce the precision of floating-point numbers, which normally occupy 32 bits, to integers of 8 or even 4 bits.\n",
        "\n",
        "This reduction occurs in the modelâ€™s parameters, specifically in the weights of the neural layers, and in the activation values that flow through the modelâ€™s layers.\n",
        "\n",
        "This means that we not only achieve an improvement in the modelâ€™s storage size and memory consumption but also greater agility in its calculations.\n",
        "\n",
        "Naturally, there is a loss of precision, but particularly in the case of 8-bit quantization, this loss is minimal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyoaSeEAi8_W"
      },
      "source": [
        "## Let's see a example of a quantized number.\n",
        "\n",
        "In reality, what I want to examine is the precision loss that occurs when transitioning from a 32-bit number to a quantized 8/4-bit number and then returning to its original 32-bit value.\n",
        "\n",
        "First, I'm going to create a function to quantize and another to unquantize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj5-xG8WogNP"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes --quiet\n",
        "!apt-get install --quiet\n",
        "!pip install -U datasets --quiet\n",
        "!pip uninstall -y peft transformers accelerate trl bitsandbytes\n",
        "!pip install -q accelerate==0.29.3\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q accelerate==0.29.3\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q trl==0.8.6\n",
        "!pip install -q peft==0.10.0\n",
        "!pip install \"transformers>=4.41.0,<5.0.0\" --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio triton transformers datasets accelerate trl\n",
        "\n",
        "# Fresh install for Colab (CPU version; remove 'cpu' for GPU)\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install triton\n",
        "!pip install transformers datasets accelerate trl"
      ],
      "metadata": {
        "id": "0PeICiYioPZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers trl bitsandbytes"
      ],
      "metadata": {
        "id": "hofHdhWOqJE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necesary linbraries\n",
        "import numpy as np\n",
        "import math\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "bbYMxu7EnWVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k17vVVU9iKws"
      },
      "outputs": [],
      "source": [
        "#Functions to quantize and unquantize\n",
        "def quantize(value, bits=4):\n",
        "    quantized_value = np.round(value * (2**(bits - 1) - 1))\n",
        "    return int(quantized_value)\n",
        "\n",
        "def unquantize(quantized_value, bits=4):\n",
        "    value = quantized_value / (2**(bits - 1) - 1)\n",
        "    return float(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtGC-Mhh3nH"
      },
      "source": [
        "Quatizied values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXRJ7mJMlpjA"
      },
      "outputs": [],
      "source": [
        "quant_4 = quantize(0.622, 4)\n",
        "print (quant_4)\n",
        "quant_8 = quantize(0.622, 8)\n",
        "print(quant_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN7K4714h8S8"
      },
      "source": [
        "Unquantized values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y50sGnmbmCqv"
      },
      "outputs": [],
      "source": [
        "unquant_4 = unquantize(quant_4, 4)\n",
        "print(unquant_4)\n",
        "unquant_8 = unquantize(quant_8, 8)\n",
        "print(unquant_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjuQtfFTalb"
      },
      "source": [
        "If we consider that the original number was 0.622, it can be said that 8-bit quantization barely loses precision, and the loss from 4-bit quantization is manageable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzCAXBmMnNSA"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-1, 1, 50)\n",
        "y = [math.cos(val) for val in x]\n",
        "\n",
        "\n",
        "y_quant_8bit = np.array([quantize(val, bits=8) for val in y])\n",
        "y_unquant_8bit = np.array([unquantize(val, bits=8) for val in y_quant_8bit])\n",
        "\n",
        "y_quant_4bit = np.array([quantize(val, bits=4) for val in y])\n",
        "y_unquant_4bit = np.array([unquantize(val, bits=4) for val in y_quant_4bit])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKYNb0nMjWWu"
      },
      "source": [
        "Letâ€™s plot a curve with the unquantized values of a cosine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u35LgstBoaTQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 12))\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(x, y, label=\"Original\")\n",
        "plt.plot(x, y_unquant_8bit, label=\"unquantized_8bit\")\n",
        "plt.plot(x, y_unquant_4bit, label=\"unquantized_4bit\")\n",
        "plt.legend()\n",
        "plt.title(\"Quantized Curves Graph Comparision\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLv-uYtlT2Bo"
      },
      "source": [
        "As you can see, the difference between the 8-bit and the original values is minimal. However, we need to use 4-bit quantization if we want to load the 7B Model into a 16GB GPU without problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620MdVMk7iUS"
      },
      "source": [
        "\n",
        "# QLoRA. Fine-tuning a 4-bit Quantized Model using LoRA.\n",
        "We are going to fine-tune with LoRA a 7B Model Quantizated to 4 bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uml3wgdri2_X"
      },
      "source": [
        "## Load the PEFT and Datasets Libraries.\n",
        "\n",
        "The PEFT library contains the Hugging Face implementation of differente fine-tuning techniques, like LoRA Tuning.\n",
        "\n",
        "Using the Datasets library we have acces to a huge amount of Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the librairies version before download or adjustments"
      ],
      "metadata": {
        "id": "hQe244tKakyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "def check_version(lib_name):\n",
        "    try:\n",
        "        lib = importlib.import_module(lib_name)\n",
        "        version = getattr(lib, '__version__', 'â“ Version not found')\n",
        "        print(f\"{lib_name}: âœ… Installed, version {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"{lib_name}: âŒ Not installed\")\n",
        "\n",
        "\n",
        "check_version(\"accelerate\")\n",
        "check_version(\"bitsandbytes\")\n",
        "check_version(\"trl\")\n",
        "check_version(\"peft\")\n",
        "check_version(\"transformers\")\n",
        "check_version(\"datasets\")"
      ],
      "metadata": {
        "id": "WbUgJNKxagnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuYNBSmTbvtB"
      },
      "source": [
        "I'm going to download the peft and Transformers libraries from their repositories on GitHub instead of using pip. This is not strictly necessary, but this way, you can get the newest versions of the libraries with support for newer models. If you want to check one of the latest models, you can use this trick.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVe5LcY9deQ5"
      },
      "outputs": [],
      "source": [
        "#Install the lastest versions of peft & transformers library recommended\n",
        "#if you want to work with the most recent models\n",
        "#!pip install -q git+https://github.com/huggingface/peft.git\n",
        "#!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "#!pip install -q git+https://github.com/huggingface/accelerate.git\n",
        "#!pip install -q git+https://github.com/huggingface/trl.git\n",
        "#!pip install --upgrade datasets gcsfs\n",
        "#!pip install fsspec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOnJlBY-81Wl"
      },
      "source": [
        "From the Transformers library, we import the necessary classes to load the model and the tokenizer.\n",
        "\n",
        "The notebook is ready to work with different Models I tested it with models from the Bloom Family and Llama-3.\n",
        "\n",
        "I recommend you to test different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bc5eKi_efxO"
      },
      "source": [
        "## Hugging Face login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHzVpaYMfVde"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwAiEFifgp3-"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vziwd2UuCYGl"
      },
      "outputs": [],
      "source": [
        "#Use any model you want, if you want to do some fast test, just use the smallest one.\n",
        "\n",
        "#model_name = \"bigscience/bloomz-560m\"\n",
        "#model_name=\"bigscience/bloom-1b1\"\n",
        "#model_name = \"bigscience/bloom-7b1\"\n",
        "#target_modules = [\"query_key_value\"]\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "target_modules = [\"query_key_value\"] #YOU MAY CHANGE THIS BASED ON YOUR MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVF_hKiPd1lh"
      },
      "source": [
        "To load the model, we need a configuration class that specifies how we want the quantization to be performed. Weâ€™ll achieve this with the BitesAndBytesConfig from the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h_ydWGf6EAd"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x47-cqrXosTc"
      },
      "source": [
        "We are specifying the use of 4-bit quantization and also enabling double quantization to reduce the precision loss.\n",
        "\n",
        "For the bnb_4bit_quant_type parameter, I've used the recommended value in the paper [QLoRA: Efficient Finetuning of Quantized LLMs.](https://arxiv.org/abs/2305.14314)\n",
        "\n",
        "Now, we can go ahead and load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2EZhNQ66EAd"
      },
      "outputs": [],
      "source": [
        "device_map = {\"\": 0}\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                    quantization_config=bnb_config,\n",
        "                    device_map=device_map,\n",
        "                    use_cache = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVDiZYbee77R"
      },
      "source": [
        "Now we have the quantized version of the model in memory. Yo can try to load the unquantized version to see if it's possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU0awofs84q7"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Inference with the pre-trained model.\n",
        "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jak6FzpvFTHk"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):#PLAY WITH ARGS AS YOU SEE FIT\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=False, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkFqjS459jAa"
      },
      "source": [
        "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
        "\n",
        "I'm going to request the pre-trained model that acts like a motivational coach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BAYg7czFYeK"
      },
      "outputs": [],
      "source": [
        "#Inference original model\n",
        "input_sentences = tokenizer(\"acts like a motivational coach\", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(\"As a motivational coach, you recommand me : \", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "N6NdGZUzcYQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUGY47p9ysI"
      },
      "source": [
        "The answer is good enough, the models used is a really well trained Model. But we will try to improve the quality with a sort fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5L_DcR9ggA"
      },
      "source": [
        "## Preparing the Dataset.\n",
        "The Dataset useds is:\n",
        "\n",
        "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# This will remove your local datasets cache (be careful: deletes ALL datasets cache!)\n",
        "shutil.rmtree(os.path.expanduser(\"~/.cache/huggingface/datasets\"), ignore_errors=True)"
      ],
      "metadata": {
        "id": "8Fp08alSmm8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyIMQ7IHFbIx"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(50))\n",
        "\n",
        "del data\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmlZY3fk_9fm"
      },
      "outputs": [],
      "source": [
        "print(train_sample[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPAJsrUAHiJ"
      },
      "source": [
        "## Fine-Tuning.\n",
        "The first step will be to create a LoRA configuration object where we will set the variables that specify the characteristics of the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCalslQFGL7K"
      },
      "outputs": [],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "lora_config = LoraConfig(\n",
        "    r=16, #As bigger the R bigger the parameters to train.\n",
        "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix. It seems that as higher more weight have the new training.\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
        "    bias=\"none\", # this specifies if the bias parameter should be trained.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUddynl0B1Ck"
      },
      "source": [
        "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the value more parameters are trained, but it means that the model will be able to learn more complicated relations between inputs and outputs.\n",
        "\n",
        "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
        "\n",
        "**lora_alpha**. Ad bigger the number more weight have the LoRA activations, it means that the fine-tuning process will have more impac as bigger is this value.\n",
        "\n",
        "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
        "\n",
        "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
        "\n",
        "**task_type**. Indicates the task the model is beign trained for. In this case, text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HArPQ_lvGUkY"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWalmqWm4STo"
      },
      "source": [
        "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND0aJ-t6ARqD"
      },
      "outputs": [],
      "source": [
        "#Creating the TrainingArgs\n",
        "import transformers\n",
        "from transformers import TrainingArguments # , Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True, # Find a correct batch size that fits the size of Data.\n",
        "    learning_rate= 2e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show trl"
      ],
      "metadata": {
        "id": "gL-XPitkczog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxsV-iy_J_o"
      },
      "source": [
        "Now we can train the model.\n",
        "To train the model we need:\n",
        "\n",
        "\n",
        "*   The Model.\n",
        "*   The training_args\n",
        "* The Dataset\n",
        "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
        "* The LoRA config.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample = train_sample.rename_column(\"prompt\", \"text\")"
      ],
      "metadata": {
        "id": "YESO9Vn5c6Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y trl\n",
        "!pip install git+https://github.com/huggingface/trl.git"
      ],
      "metadata": {
        "id": "DsLHPBsJc7_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "help(SFTTrainer)"
      ],
      "metadata": {
        "id": "Fd_gp86Qc-9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5NYHqBnGZyF"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    peft_config = lora_config,\n",
        "    #dataset_text_field=\"text\",\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEKiFdpDGgOx"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Hmy87-HViP"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glDE0Z4FOSe0"
      },
      "outputs": [],
      "source": [
        "#In case you are having memory problems uncomment this lines to free some memory\n",
        "import gc\n",
        "import torch\n",
        "del foundation_model\n",
        "del trainer\n",
        "del train_sample\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOAqEg0mSjHW"
      },
      "source": [
        "## Inference with the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX5d8xMCSC6y"
      },
      "outputs": [],
      "source": [
        "#import peft\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
        "#import os\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA-E-io_Dfe1"
      },
      "outputs": [],
      "source": [
        "bnb_config2 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TAjrSWSe14q"
      },
      "outputs": [],
      "source": [
        "#Load the Model.\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        peft_model_path,\n",
        "                                        #torch_dtype=torch.bfloat16,\n",
        "                                        is_trainable=False,\n",
        "                                        #load_in_4bit=True,\n",
        "                                        quantization_config=bnb_config2,\n",
        "                                        device_map = 'cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK--YFPR6OxH"
      },
      "source": [
        "## Inference the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_27uvJudf03"
      },
      "outputs": [],
      "source": [
        "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCV9JOBG6Ug8"
      },
      "source": [
        "The result is really good. Let's compare the answer of the pre-trained model with the fine-tuned one:\n",
        "\n",
        "* **Pretrained Model**: 'I want you to act as a motivational coach. \\xa0You are going on an adventure with me, and I need your help.\\nWe will be traveling through the land of â€œWhat If.â€ \\xa0 This is not some place that exists in reality; itâ€™s more like one those places we see when watching'\n",
        "\n",
        "* **Fine-Tuned Model**: 'I want you to act as a motivational coach.  I will provide some information about an individual or group of people who need motivation, and your role is help them find the inspiration they require in order achieve their goals successfully! You can use techniques such as positive reinforcement, visualization exercises etc., depending on what'\n",
        "\n",
        "As you can see, the result is really similar to the samples contained in the dataset used to fine-tune the model. And we only trained the model for some epochs and with a really small number of rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq0ZeeeyepF-"
      },
      "source": [
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try a few versions if you have time\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring with a bigger model: bloom-1b1"
      ],
      "metadata": {
        "id": "1oeTl400dcFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Update model name and target modules"
      ],
      "metadata": {
        "id": "5NpUZPSRdh03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bigscience/bloom-1b1\"\n",
        "target_modules = [\"query_key_value\"]"
      ],
      "metadata": {
        "id": "kJ1FhYhmdn6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Reload tokenizer & model with quantization"
      ],
      "metadata": {
        "id": "7C6SsaMBdrmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # \"auto\" may better balance layers if VRAM is tight\n",
        "    use_cache=False\n",
        ")"
      ],
      "metadata": {
        "id": "4fkg2wwvdv75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. LoRA Config"
      ],
      "metadata": {
        "id": "6wfLDW5Dd5UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "X_SpNG8Dd531"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Reload and Adjust training sample size"
      ],
      "metadata": {
        "id": "L2tKAEZXeBX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "#Create the Dataset to create prompts.\n",
        "data = load_dataset(dataset)\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(20)) # downsize for test run\n",
        "\n",
        "del data\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ],
      "metadata": {
        "id": "h-LGP4fDeDF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. SFTTrainer"
      ],
      "metadata": {
        "id": "Izn1z4ESeJ7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,             # quantized base model\n",
        "    args=training_args,                 # TrainingArguments (learning rate, epochs, etc.)\n",
        "    train_dataset=train_sample,         # your tokenized dataset\n",
        "    peft_config=lora_config,            # LoRA config\n",
        "    processing_class=tokenizer,         # required instead of tokenizer=...\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
        "        tokenizer, mlm=False            # causal LM task = NOT masked LM\n",
        "    )\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Q0CHBWB8eLTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the model"
      ],
      "metadata": {
        "id": "UkkVkDlHeOpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ],
      "metadata": {
        "id": "mPZgWKy-eT5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model_bloom-1b1\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "gdvEfJKFeYIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Reload Fine-Tuned bloom-1b1 with LoRA (4-bit QLoRA)"
      ],
      "metadata": {
        "id": "SSHVNNf4ejeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import peft\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
        "#import os\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model_bloom-1b1\")"
      ],
      "metadata": {
        "id": "CUUTj3vgelHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config2 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "oa3OW-GsepHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Model.\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        peft_model_path,\n",
        "                                        #torch_dtype=torch.bfloat16,\n",
        "                                        is_trainable=False,\n",
        "                                        #load_in_4bit=True,\n",
        "                                        quantization_config=bnb_config2,\n",
        "                                        device_map = 'cuda')"
      ],
      "metadata": {
        "id": "HrBmYH0besgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference the fine-tuned model."
      ],
      "metadata": {
        "id": "XBHUilx2ewPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "HWgRahBoexVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring with a bigger model: mistralai/Mistral-7B-v0.1"
      ],
      "metadata": {
        "id": "6PyYMiIBe3Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fsspec==2025.3.2\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/trl.git\n",
        "!pip install -q bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "N5xa_VdFe3wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import torch"
      ],
      "metadata": {
        "id": "hPR4U47Ue9VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt user to input HF token (hidden input)\n",
        "hf_token = getpass(\"ðŸ” Enter your Hugging Face token: \")\n",
        "\n",
        "login(token=hf_token)\n",
        "print(\"âœ… Logged in to Hugging Face.\")"
      ],
      "metadata": {
        "id": "HKFbZOVYe_BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Model name and target modules"
      ],
      "metadata": {
        "id": "37M9JV00fC_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]"
      ],
      "metadata": {
        "id": "IAJuxXqOfD4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenizer & model with quantization"
      ],
      "metadata": {
        "id": "F8H8CGFrfF-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # \"auto\" may better balance layers if VRAM is tight\n",
        "    use_cache=False,\n",
        "    use_auth_token=True\n",
        ")"
      ],
      "metadata": {
        "id": "beYwx5jMfK5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. LoRA config"
      ],
      "metadata": {
        "id": "xxphvKUKfRp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "OSM1FvxGfTZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Reload and Adjust training sample size"
      ],
      "metadata": {
        "id": "6EeEnsxwfVog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "#Create the Dataset to create prompts.\n",
        "data = load_dataset(dataset)\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(20)) # downsize for test run\n",
        "\n",
        "del data\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ],
      "metadata": {
        "id": "1ciuKL1wfYwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting TrainingArguments"
      ],
      "metadata": {
        "id": "XWHJEAMOfcfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft_lab_outputs/checkpoints_mistral\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=5,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # use bf16=True if you're on A100 with bfloat16\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZcBBdaMMfizB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. SFTTrainer"
      ],
      "metadata": {
        "id": "CaWVt1qWfnxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,             # quantized base model\n",
        "    args=training_args,                 # TrainingArguments (learning rate, epochs, etc.)\n",
        "    train_dataset=train_sample,         # your tokenized dataset\n",
        "    peft_config=lora_config,            # LoRA config\n",
        "    processing_class=tokenizer,         # required instead of tokenizer=...\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)          # causal LM task = NOT masked LM\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ko8VVbVXfoac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the model"
      ],
      "metadata": {
        "id": "qheFERlWft5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ],
      "metadata": {
        "id": "JpV3engDfv1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model_Mistral-7B\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "LLKmrcovfwaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reload Fine-Tuned mistralai/Mistral-7B-v0.1 with LoRA (4-bit QLoRA)"
      ],
      "metadata": {
        "id": "Gu9oueM-f3oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import peft\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
        "#import os\n",
        "\n",
        "device_map = \"auto\"\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model_Mistral-7B\")"
      ],
      "metadata": {
        "id": "eEEDRnzDfyb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config2 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "Wzn3dQFNf-Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Model.\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        peft_model_path,\n",
        "                                        #torch_dtype=torch.bfloat16,\n",
        "                                        is_trainable=False,\n",
        "                                        #load_in_4bit=True,\n",
        "                                        quantization_config=bnb_config2,\n",
        "                                        device_map = 'auto')"
      ],
      "metadata": {
        "id": "ufExnMAVgApf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define outputs function for Mistral-7B model"
      ],
      "metadata": {
        "id": "k4fLLdGzgFCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_outputs(model, inputs, max_new_tokens=100):\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs.get(\"attention_mask\", None),\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,                # Enable sampling for more creative output\n",
        "        top_p=0.95,                    # Nucleus sampling\n",
        "        temperature=0.7,               # Control randomness\n",
        "        repetition_penalty=1.2,        # Lower than 1.5 to avoid over-penalizing\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,  # Required for some decoding\n",
        "    )\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "u8G0TFgVgF0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference the fine-tuned model."
      ],
      "metadata": {
        "id": "gSmxCKi3gHpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "qAkGRwnsgNje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference the fine-tuned model with longer completion."
      ],
      "metadata": {
        "id": "YlIFWxMcgS4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Act as a startup advisor.\",\n",
        "    \"You are a helpful travel planner.\",\n",
        "    \"Pretend you're a Shakespearean poet.\",\n",
        "]"
      ],
      "metadata": {
        "id": "o-kAXMJ_gTeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(prompts[0], return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "D41IWfPKgXrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(prompts[1], return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "iokwN_zSgah-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(prompts[2], return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "u9cQbYkrgd1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batch multiple prompts for faster inference:"
      ],
      "metadata": {
        "id": "NgZ_v-Asgj7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to('cuda')\n",
        "outputs = get_outputs(loaded_model, inputs, max_new_tokens=60)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "sjyvsTeqgkqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My observation\n",
        "\n",
        "My Lab Feedback â€” Exploring LoRA Fine-Tuning with Larger Models\n",
        "\n",
        "This lab was particularly engaging because it gave me hands-on experience with scaling instruction tuning using more capable base models like bloom-1b1 and mistralai/Mistral-7B-v0.1, all while keeping memory usage low via 4-bit quantization.\n",
        "\n",
        "I started by revisiting the bloom-1b1 architecture, adapting my LoRA target modules and confirming that quantization + LoRA still provided coherent completions. Even on a tiny training set, the fine-tuned model returned contextual responses aligned with the intended prompts. The process helped solidify my understanding of how instruction-tuned models learn patterns even with minimal updates.\n",
        "\n",
        "Then I moved on to Mistral-7B, which required slightly more care due to access restrictions and increased resource demands. Switching to a GPU A100 and handling gated repo access manually reminded me of real-world deployment constraints. Once configured, Mistralâ€™s performance was impressive â€” it generated stylistically rich and relevant completions across various prompts (startup advice, travel planning, and poetry) even after just a few training epochs.\n",
        "\n",
        "Key takeaways for me:\n",
        "- I now feel confident setting up and training LoRA adapters on quantized models.\n",
        "- I learned how to troubleshoot common issues (HF token gating, session resets, CUDA memory balancing).\n",
        "- It was insightful to see how larger base models can produce much more expressive text with only lightweight tuning.\n",
        "- Batch inference and prompt engineering clearly play a major role in how effective the final model is during deployment.\n",
        "\n",
        "This lab didnâ€™t just deepen my understanding of fine-tuning â€” it also gave me a reusable workflow for testing other Hugging Face LLMs with LoRA adapters in resource-constrained environments. Iâ€™m looking forward to applying this to more realistic datasets and exploring evaluation metrics next.\n",
        "\n"
      ],
      "metadata": {
        "id": "UxUH4bbfgo6E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "445e694e"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}